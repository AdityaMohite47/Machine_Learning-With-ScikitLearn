{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Polynomial Features , And Regularization with Cross-Validation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data Prepration   \n",
    "data = pd.read_csv(\"./Datasets/Advertising.csv\")\n",
    "\n",
    "X = data.drop('sales' , axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial features through sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_converter = PolynomialFeatures(degree=2 , include_bias=False)\n",
    "X = poly_converter.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y ,test_size=0.3 , random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some Notes :\n",
    "  - constant in the equation of any algorithm which is a tunable hyper-parameters is refered as alpha in sklearn. \n",
    "\n",
    "\n",
    "  - The Cross-Validation uses the scoring param which is used to \n",
    " find the best hyper-param value for a model And the higher the value of the test the better the model is.\n",
    "\n",
    "\n",
    "  - This is to maintain uniformity across all scoring metrics.\n",
    " A higher accuracy is better but a higher RMSE is worse. Sklearn fixes it by using -ve RMSE as its scorer metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- l2 Eegularization / Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.08187241  0.64602581  0.01404022 -0.24228323  2.70757796  0.20338707\n",
      "  0.32854627  0.18371122 -0.18521417]\n",
      "1.0342971166091748\n",
      "0.1\n",
      "0.7159217915840503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge , RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ridge_model = Ridge(alpha=10)\n",
    "ridge_model.fit(X_train , y_train)\n",
    "\n",
    "print(ridge_model.coef_)\n",
    "\n",
    "test_pred = ridge_model.predict(X_test)\n",
    "\n",
    "# RMSE of the base model\n",
    "print(np.sqrt(mean_squared_error(y_test , test_pred)))\n",
    "\n",
    "# Performing cross-validation for the best alpha\n",
    "\n",
    "ridge_cv_model = RidgeCV(alphas=(0.1 , 0.4 , 0.9 , 1.0 , 10.0) , scoring='neg_root_mean_squared_error')\n",
    "ridge_cv_model.fit(X_train , y_train)\n",
    "\n",
    "print(ridge_cv_model.alpha_) # 0.1 was the best performing alpha   \n",
    "\n",
    "test_pred = ridge_cv_model.predict(X_test)\n",
    "# RMSE of the cv model\n",
    "print(np.sqrt(mean_squared_error(y_test , test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 Regularization / Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6900986878105823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv_model = LassoCV(eps=0.001 , n_alphas=100 , cv=8)\n",
    "\n",
    "# same as np.linspace eps/epsilon acts as starting point and n_alphas as\n",
    "# no. of values to discover further\n",
    "\n",
    "lasso_cv_model.fit(X_train , y_train)\n",
    "\n",
    "test_pred = lasso_cv_model.predict(X_test)\n",
    "\n",
    "# RMSE of the lasso cv model\n",
    "print(np.sqrt(mean_squared_error(y_test , test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elastic Net Regularization : L1 + L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.005471702007194302\n",
      "0.6900986878105823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# refers to                         alpha-ratio of l1:l2                  lambda values to consider\n",
    "elastic_cv_model = ElasticNetCV(l1_ratio=[.1 , .3 , .5 , .7 , .9 , 1] , eps=0.001 , n_alphas=100)\n",
    "\n",
    "elastic_cv_model.fit(X_train , y_train)\n",
    "\n",
    "print(elastic_cv_model.l1_ratio_) # 0.9 \n",
    "print(elastic_cv_model.alpha_)\n",
    "\n",
    "test_pred = lasso_cv_model.predict(X_test)\n",
    "\n",
    "# RMSE of the elastic cv model\n",
    "print(np.sqrt(mean_squared_error(y_test , test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note : for small datasets elasticnet_model decides to stay with lasso only i.e alpha ratio doesn't matter for small datasets in elasticnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
